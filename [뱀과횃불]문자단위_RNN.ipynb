{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freud-sensei/imfine_torch/blob/main/%5B%EB%B1%80%EA%B3%BC%ED%9A%83%EB%B6%88%5D%EB%AC%B8%EC%9E%90%EB%8B%A8%EC%9C%84_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 간단한 예제"
      ],
      "metadata": {
        "id": "tubttEYlMcbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hONA_dFVfaKN"
      },
      "outputs": [],
      "source": [
        "# 얘네 기능은 이제 다 알 거라고 믿음\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련데이터 전처리하기"
      ],
      "metadata": {
        "id": "WwRxhQ2uh2dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "목표: `apple`을 입력받으면 `pple!`을 출력받는다."
      ],
      "metadata": {
        "id": "Z490C57sh5CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자집합 만들기\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str + label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print(vocab_size) # 문자집합의 크기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9kUyo12iF_O",
        "outputId": "d84a40de-f6f3-4b4f-b50f-d503a4b69743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫 벡터 사용: 입력 크기는 문자집합의 크기\n",
        "input_size = vocab_size # time step 수가 '아님!!!\"\"\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "ni0jijTiiQ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱싱\n",
        "char_to_index = dict((c, idx) for idx, c in enumerate(char_vocab))\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vffe1RkYic2C",
        "outputId": "904150b7-f51d-41d2-f136-44d994cca242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char = dict((value, key) for key, value in char_to_index.items())\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibGfD-Fsi2wd",
        "outputId": "ad0fabf5-0e90-44cd-f987-0f633a416245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e5Ow5HcjETf",
        "outputId": "cfa265df-cf24-4233-874b-b5bb8251133b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.RNN()은 기본적으로 3차원 텐서를 입력받는다.\n",
        "# 즉 차원을 하나 추가해주자. (나머지 한 차원은 임베딩을 통해 추가될 것임...)\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFyevvtojREx",
        "outputId": "9dec2edd-6198-49eb-9889-a439a0c096d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫 벡터로 바꿔주기\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "id": "D4dh1tcOjcH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4445a65-62de-4719-d694-2a7ca520a407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "y = torch.LongTensor(y_data)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxnpYnvIKC8A",
        "outputId": "b68d4f8d-cb8a-459a-fb62-8b75116c91be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n",
            "torch.Size([1, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6f867671dc43>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구현하기"
      ],
      "metadata": {
        "id": "cEiOY3Y-KQh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    # x: 모든 timesteps의 은닉 상태 (배치크기, timesteps 수, 은닉상태의 크기)\n",
        "    # _status: 마지막 timestep의 은닉 상태, 대신 모든 layer 해당 (층의 개수, 배치크기, 은닉상태의 크기)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "X9U-uSJ7KRqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)\n",
        "y_pred = net(X)\n",
        "print(y_pred.shape) # (배치크기, 시점수, 출력의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2vLsEGCLmcw",
        "outputId": "cfddff86-5ae5-42bc-c544-64138fec9db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도를 측정할 때는 배치, 시점 차원을 하나로 만들어야 합니다.\n",
        "print(y_pred.view(-1, input_size).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGFZZ3FZLxGj",
        "outputId": "8f367c50-b19b-490f-c974-084958817a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 데이터의 크기\n",
        "print(y.shape)\n",
        "print(y.view(-1).shape) # 1차원 벡터로 바꾸겠다는 소리에요"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPeHA3EUL-HP",
        "outputId": "0a0dfcdf-47f6-4d9d-fae6-5ff2ef23069f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "ZxpaW2vIMKmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습하기"
      ],
      "metadata": {
        "id": "yJ6dYZ-vMXG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num of epochs = 100\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = net(X)\n",
        "  loss = loss_function(y_pred.view(-1, input_size), y.view(-1)) # batch 차원 제거\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 어떻게 예측했는지 확인\n",
        "  result = y_pred.detach().numpy().argmax(axis=2) # 각 time-step별 5차원 벡터에서, 가장 높은 인덱스값 선택\n",
        "  result_str = ''.join(index_to_char[idx] for idx in result.reshape(-1))\n",
        "  print(f\"epoch {i}, loss {loss.item()}\")\n",
        "  print(f\"predicton {result}, true y {result}\")\n",
        "  print(f\"prediction str {result_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZwiysJFMfPU",
        "outputId": "97180ab5-fad3-48bc-db4c-3d9075d413dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 1.3217504024505615\n",
            "predicton [[4 4 4 0 0]], true y [[4 4 4 0 0]]\n",
            "prediction str ppp!!\n",
            "epoch 1, loss 1.1565921306610107\n",
            "predicton [[4 4 4 4 0]], true y [[4 4 4 4 0]]\n",
            "prediction str pppp!\n",
            "epoch 2, loss 1.0058705806732178\n",
            "predicton [[4 4 4 2 0]], true y [[4 4 4 2 0]]\n",
            "prediction str pppe!\n",
            "epoch 3, loss 0.8865023851394653\n",
            "predicton [[4 4 4 2 2]], true y [[4 4 4 2 2]]\n",
            "prediction str pppee\n",
            "epoch 4, loss 0.8021324276924133\n",
            "predicton [[4 4 4 2 2]], true y [[4 4 4 2 2]]\n",
            "prediction str pppee\n",
            "epoch 5, loss 0.7323900461196899\n",
            "predicton [[4 4 4 2 2]], true y [[4 4 4 2 2]]\n",
            "prediction str pppee\n",
            "epoch 6, loss 0.6769901514053345\n",
            "predicton [[4 4 4 2 2]], true y [[4 4 4 2 2]]\n",
            "prediction str pppee\n",
            "epoch 7, loss 0.6306073665618896\n",
            "predicton [[4 4 4 2 0]], true y [[4 4 4 2 0]]\n",
            "prediction str pppe!\n",
            "epoch 8, loss 0.5960450172424316\n",
            "predicton [[4 4 4 0 0]], true y [[4 4 4 0 0]]\n",
            "prediction str ppp!!\n",
            "epoch 9, loss 0.5526713728904724\n",
            "predicton [[4 4 4 0 0]], true y [[4 4 4 0 0]]\n",
            "prediction str ppp!!\n",
            "epoch 10, loss 0.4818086624145508\n",
            "predicton [[4 4 4 2 0]], true y [[4 4 4 2 0]]\n",
            "prediction str pppe!\n",
            "epoch 11, loss 0.4584383964538574\n",
            "predicton [[4 4 4 2 0]], true y [[4 4 4 2 0]]\n",
            "prediction str pppe!\n",
            "epoch 12, loss 0.3638309836387634\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 13, loss 0.3340739607810974\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 14, loss 0.28841108083724976\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 15, loss 0.2231869399547577\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 16, loss 0.19843368232250214\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 17, loss 0.18218150734901428\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 18, loss 0.13758422434329987\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 19, loss 0.13146229088306427\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 20, loss 0.12311825901269913\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 21, loss 0.09711456298828125\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 22, loss 0.08596383780241013\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 23, loss 0.08670765906572342\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 24, loss 0.06707678735256195\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 25, loss 0.05881618708372116\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 26, loss 0.05862541124224663\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 27, loss 0.05067652463912964\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 28, loss 0.039931975305080414\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 29, loss 0.03711434453725815\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 30, loss 0.036805160343647\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 31, loss 0.029133141040802002\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 32, loss 0.025122955441474915\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 33, loss 0.02495933696627617\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 34, loss 0.02335900440812111\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 35, loss 0.01955745369195938\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 36, loss 0.016545847058296204\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 37, loss 0.015883859246969223\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 38, loss 0.015652740374207497\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 39, loss 0.013600071892142296\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 40, loss 0.011653216555714607\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 41, loss 0.01087208092212677\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 42, loss 0.010594230145215988\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 43, loss 0.010122997686266899\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 44, loss 0.009284568950533867\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 45, loss 0.008348311297595501\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 46, loss 0.007643584161996841\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 47, loss 0.007289734669029713\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 48, loss 0.007137283682823181\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 49, loss 0.006885833106935024\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 50, loss 0.006418770644813776\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 51, loss 0.0059156594797968864\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 52, loss 0.00554764736443758\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 53, loss 0.005324439145624638\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 54, loss 0.005171214230358601\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 55, loss 0.0050130062736570835\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 56, loss 0.004813978914171457\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 57, loss 0.004582731984555721\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 58, loss 0.004354517441242933\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 59, loss 0.004164580255746841\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 60, loss 0.004028039984405041\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 61, loss 0.003932804800570011\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 62, loss 0.0038467925041913986\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 63, loss 0.0037404235918074846\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 64, loss 0.0036093792878091335\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 65, loss 0.003473131451755762\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 66, loss 0.0033535431139171124\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 67, loss 0.003258523065596819\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 68, loss 0.0031825657933950424\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 69, loss 0.003114303108304739\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 70, loss 0.0030438851099461317\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 71, loss 0.0029673369135707617\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 72, loss 0.0028871092945337296\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 73, loss 0.002809236291795969\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 74, loss 0.002739403862506151\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 75, loss 0.002679957542568445\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 76, loss 0.002628691028803587\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 77, loss 0.0025806170888245106\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 78, loss 0.0025310716591775417\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 79, loss 0.0024782707914710045\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 80, loss 0.0024243127554655075\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 81, loss 0.0023723591584712267\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 82, loss 0.0023248635698109865\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 83, loss 0.002282124711200595\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 84, loss 0.002242856426164508\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 85, loss 0.0022049203980714083\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 86, loss 0.002166916150599718\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 87, loss 0.00212848954834044\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 88, loss 0.0020902869291603565\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 89, loss 0.00205330946482718\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 90, loss 0.002018441678956151\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 91, loss 0.0019858081359416246\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 92, loss 0.001955080311745405\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 93, loss 0.0019253346836194396\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 94, loss 0.0018958600703626871\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 95, loss 0.0018665129318833351\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 96, loss 0.0018374333158135414\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 97, loss 0.0018090460216626525\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 98, loss 0.0017817281186580658\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n",
            "epoch 99, loss 0.001755572622641921\n",
            "predicton [[4 4 3 2 0]], true y [[4 4 3 2 0]]\n",
            "prediction str pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 복잡한? 예제"
      ],
      "metadata": {
        "id": "IYonc2mWNr8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 데이터 전처리하기"
      ],
      "metadata": {
        "id": "9WV-jGjuOCkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "fFW5p-hGOCZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "7ORc7PhXOScC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence))\n",
        "char_to_idx = dict((char, idx) for idx, char in enumerate(char_set))\n",
        "print(char_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0taIVIMhOWyU",
        "outputId": "31c360f9-30f8-4276-c4d6-d4b6c0e3ae36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'y': 0, 's': 1, ' ': 2, \"'\": 3, '.': 4, 'o': 5, 'c': 6, 'k': 7, 'f': 8, 'n': 9, ',': 10, 'i': 11, 'l': 12, 'a': 13, 'b': 14, 'm': 15, 'h': 16, 'r': 17, 'e': 18, 'u': 19, 't': 20, 'w': 21, 'p': 22, 'g': 23, 'd': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_size = len(char_to_idx) # 문자집합의 크기\n",
        "print(dict_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuYc2umoOnK9",
        "outputId": "911779de-2b2f-4ce1-a352-1acff2bcdbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dict_size\n",
        "sequence_length = 10 # 10의 단위로 샘플들을 잘라서 데이터를 만들어보자\n",
        "learning_rate = .1"
      ],
      "metadata": {
        "id": "xQMK1hjnOspj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 만들기\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "i = 0\n",
        "while i + sequence_length < len(sentence):\n",
        "  X_str = sentence[i:i + sequence_length]\n",
        "  y_str = sentence[i + 1:i + sequence_length + 1]\n",
        "  print(i, X_str, '->' , y_str)\n",
        "\n",
        "  X_data.append([char_to_idx[char] for char in X_str])\n",
        "  y_data.append([char_to_idx[char] for char in y_str])\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXmfJKYCO5mw",
        "outputId": "17a9982c-fb68-4ed1-af2c-e5eb4fe62b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diYhDa-RPbm8",
        "outputId": "3df9afd8-8356-442f-a00c-cf7065a5c743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 8, 2, 0, 5, 19, 2, 21, 13, 9]\n",
            "[8, 2, 0, 5, 19, 2, 21, 13, 9, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫인코딩\n",
        "import numpy as np\n",
        "X_one_hot = [np.eye(dict_size)[indices] for indices in X_data]\n",
        "X = torch.FloatTensor(X_one_hot)\n",
        "y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMTDqkQlPfn6",
        "outputId": "e0a6897b-ef01-4159-bde0-3d2319a2aa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-69056ab2531b>:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  X = torch.FloatTensor(X_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape) # 훈련 데이터의 크기\n",
        "print(y.shape) # 레이블의 크기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0F8X8cPsnZ",
        "outputId": "d1ed4dbe-f0c6-4aa7-87cb-a66f0e3c756b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n",
            "torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구현하기"
      ],
      "metadata": {
        "id": "yJEUgPZMPzTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
        "    # output_dim == hidden_dim == dict_size in this case\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "P5QTLziCQlD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dict_size, hidden_size, 2) # 층을 2개 쌓기\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "6AxJWREYR0Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = net(X)\n",
        "print(y_pred.shape) # (배치차원, 시점, 출력의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tOPiQxoSHG_",
        "outputId": "90458a7f-63a4-4f2a-88b4-dd02ca5015df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.view(-1, dict_size).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDFnP6orSKrA",
        "outputId": "9076390b-039f-4247-86b5-d29d0f3293b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)\n",
        "print(y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzF0uhM7StFR",
        "outputId": "e4d71f82-2c49-423e-a863-cd08e61a978b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습하기"
      ],
      "metadata": {
        "id": "hsRWXcCNTCr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = net(X)\n",
        "  loss = loss_f(y_pred.view(-1, dict_size), y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  results = y_pred.argmax(dim = 2)\n",
        "  predict_str = \"\"\n",
        "  for idx, result in enumerate(results):\n",
        "    if idx == 0:\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else:\n",
        "      predict_str += char_set[result[-1]]\n",
        "  print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XnQhwwyTAli",
        "outputId": "156b301d-1738-41c5-828a-3a6eec35fbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsco p,end to puild wod eps d r't d ut d' e  dle thgether to co le t wood tnd w g't dns gm them to ks wnd wo k, duthdether tonch toem to to gstorhtoemsnd ensetp ers tk wo toems r \n",
            "lsco p,and to poild wod eps d rkt enut dp eendle togethem to to le t woom tnd w gpt ens gm them to ks wnd wo k, duthdather tonch toem to tong tor toemsnd es  tn ins t  wo toe sor \n",
            "l co p,and to tuild wnd eps d rkt enut dp eesgle to ethem to co le t aood tnd w gpt ens gr them to ks wnd wook, duthdather tanch toem to to k tor toemsnd ec  tn ins tk ao toe sor \n",
            "l corp,and to tui d dnd eps don't dnut da ee,gle to ethem to co le t aoop tnd a gpt ens gp toem to ks dnd aook, dutodather tonch toem to tonk tor toemsnd ec  tn in  tk ao toe sorc\n",
            "l corp,and to puild and ip, do 't enut dp eefgle to ethem to co le t aook tnd aog't ens gp them tooks and dook, dut dather tonch toem to tong for toe snd ess tn insitk ao toe sor \n",
            "l corp,and to build aodhip, don't dnut du eefgle th ethec to co lect aook and aon't dns gp them to ks and aonk, dut gather torch toem to cong for toe snd ens tp itsitk ao toe s r \n",
            "l torp,and to build aodhip, dor't d um du eeogle th ethem to collect aond and aor't dnsigp them tonks tnd aonk, dut gather torch them to cong for toe snd ess tp insit  af toe sirc\n",
            "l torp,and to build aodhip, dor't d um du eeogle th ethem to bf le t aood and aor't dnsigp the  ta ks and aork, dut gather tasch them to bong for toe snd ess tn insigy af toe sory\n",
            "l torb,and to build asdhip, dor't d um dp eeople th ether to bo le t aord and aor't dnsign them taoks and aork, dut gather tooch toem to bong for toe snd ess tn insigy af themsory\n",
            "l torbwand to build and ip, aon't d um tp people to ether to co lect aord and aor't dnsigr them tas s and aork, dut dather torch them ta bong for the snd ess tn itsigy ao the siry\n",
            "l torbwand to buils and ip, don't d um tp people to ether to co lect aord and aor't dssigr them tos s and dork, dut dather torsh them to long for themsnd ess tn ensity ao themsiry\n",
            "l torbwand to luild andhip, don't dsum dp people to ether to co lect aord and aon't dssign them tosks and aork, dut gather tooch them to long tor themsnd ess tn lnsity aoithemsiry\n",
            "l cogbwand to luild and ip, don't dsum dp people together to lo lect aord and aog't dssign them tosps and dork, dut gather torch them to long tor the snd ess tc lnsity af the snry\n",
            "l cogbwand to luild andhip, don't d um dp people together to lollect aord and aon't dssign them tosks and aork, dut gather tooch them to long for the  nd ess ic ln ity tf the  ery\n",
            "l cogbwand to luild andhip, don't dsum dp people together to co lect aord and aon't dssign them tosks and aork, dut rather teach them to long for the  nd ess tc en ity tf the  ers\n",
            "l cogbwand to luild andhip, don't dnum dp people together to co lect aood and aon't dssign them tasks and aook, dut rather teach them ta long for the snd ess ic ensity tf themsnrs\n",
            "p cogbwand to build andhip, don't dnum dp people together to co lect wood and don't dssign them tooks and aook, dut dather toach them to cong for the snd ess immensity tf themsers\n",
            "p cogbwand to build andhip, don't arum dp people together to collect wood and don't assign them tasks and aook, dut rather toach them ta cong for the snd ess immensity tf the snrs\n",
            "p togmwand to build a ship, don't arum dp people together to collect wood and don't dssign them tosks and wook, dut rather toach them to long for the snd ess immensity tf the sers\n",
            "p togmwant to build a ship, don't arum dp people together to collect wood and don't assign them tosks and work, dut rather toach them ta long for the end ess immensity tf the eers\n",
            "p togmwant to build a ship, don't drum dp people together to collect word and don't assign them tasks and work, dut rather toach them ta long for the end ess immensity tf the eers\n",
            "p cogbwant to build a ship, don't drum dp people together to collect wood and don't dssign them tasks and work, dut rather toach them ta long for the sndless immensity af the sers\n",
            "p cogbwant to luild a ship, don't drum dp people together to collect wood and don't dssign them tasks and work, dut rather teach them ta long for the sndless immensity af the sers\n",
            "p togmwant to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, dut rather teach them ta long for the sndless immensity of the sers\n",
            "p togbwant to luild a ship, don't drum up people together to collect wood and don't dssign them tosks and work, dut rather toach them to long for the endless immensity of the sers\n",
            "p togmwant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, dut rather toach them ta long for the sndless immensity of the ser \n",
            "p togmwant to luild a ship, don't arum up people together to co lect wood and don't assign them tosks and work, dut rather teach them to long for the endless immensity of the eers\n",
            "p tog want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the ser \n",
            "c togmwant to luild a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sers\n",
            "c cogmwant to luild a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sers\n",
            "l bog want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the sndless immensity of the sers\n",
            "l ,og want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them ta long for the endless immensity of the eers\n",
            "p ,og want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p tou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p ,ou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the seas\n",
            "p ,ou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eeas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seas\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the soa.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the sndless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TG3ivpGQOV8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZAif2uUbi-SK"
      }
    }
  ]
}